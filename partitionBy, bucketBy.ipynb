{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ea93bb-7b90-4219-a153-2e85eae02263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a73cfab-30b3-49e1-a6e7-f76e86235dda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_df = spark.read.format(\"csv\") \\\n",
    "                    .option(\"header\",\"True\") \\\n",
    "                    .option(\"inferSchema\",\"True\") \\\n",
    "                    .option(\"mode\",\"PERMISSIVE\") \\\n",
    "                    .load(\"/FileStore/tables/Data_for_write_opt-1.csv\")\n",
    "\n",
    "read_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7985e5b-eadc-44b2-be43-06d3ffe900a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## write the data -- partitionBy diff. address\n",
    "\n",
    "read_df.write.format(\"csv\") \\\n",
    "             .option(\"header\",\"True\") \\\n",
    "             .option(\"mode\",\"overwrite\") \\\n",
    "             .option(\"path\",\"/FileStore/tables/csv_write_partition_by/\") \\\n",
    "             .partitionBy(\"address\") \\\n",
    "             .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9db0ce4e-e48c-4284-a797-dab5bf81ef14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## write the data -- partitionBy diff. address & gender\n",
    "\n",
    "read_df.write.format(\"csv\") \\\n",
    "             .option(\"header\",\"True\") \\\n",
    "             .option(\"mode\",\"overwrite\") \\\n",
    "             .option(\"path\",\"/FileStore/tables/csv_write_partition_by_check_partition/\") \\\n",
    "             .partitionBy(\"address\",\"gender\") \\\n",
    "             .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f452d1-9a6b-4fb7-b35a-67f42f7b41e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(\"/FileStore/tables/csv_write_partition_by_check_partition/\")\n",
    "df1.rdd.getNumPartitions()\n",
    "df1.select(spark_partition_id().alias('partid')).groupBy('partid').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d4e6042-17cd-4ff8-ab1e-124f7cfa6dab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/FileStore/tables/csv_write_partition_by_check_partition/address=INDIA/gender=f/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6096cd37-c5d7-41c3-85be-56e9c4bbda06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## write the data -- bucketBy diff. id\n",
    "\n",
    "read_df.write.format(\"csv\") \\\n",
    "             .option(\"header\",\"True\") \\\n",
    "             .option(\"mode\",\"overwrite\") \\\n",
    "             .option(\"path\",\"/FileStore/tables/csv_write_bucket_by_id/\") \\\n",
    "             .bucketBy(3,\"id\") \\\n",
    "             .saveAsTable(\"bucket_by_id_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "977d5d55-0fbf-4051-82a6-3b58e134245b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/FileStore/tables/csv_write_bucket_by_id\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "partitionBy, bucketBy",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}