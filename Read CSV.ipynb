{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1880569d-f722-4ace-9d50-205956419c4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba3fbc1-f7b2-4a7d-b347-85830ab41e16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flight_df  = spark.read.format('csv') \\\n",
    "                  .option(\"header\", \"true\") \\\n",
    "                  .option(\"inferSchema\", \"true\") \\\n",
    "                  .option('mode','FAILFAST') \\\n",
    "                  .load(\"/FileStore/tables/Flight_data.csv\")\n",
    "\n",
    "flight_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98078914-f18a-4421-9327-f0fbbe446c79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### READ USING OWN SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca72f2bc-3db7-4f69-84dc-9e841df4f8df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_schema = StructType(\n",
    "    [\n",
    "        StructField('DEST_COUNTRY_NAME', StringType(), True),\n",
    "        StructField('ORIGIN_COUNTRY_NAME', StringType(), True),\n",
    "        StructField('count', IntegerType(), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "flight_df_new_schema  = spark.read.format('csv') \\\n",
    "                                .option(\"header\", \"true\") \\\n",
    "                                .option(\"inferSchema\", \"false\") \\\n",
    "                                .schema(my_schema) \\\n",
    "                                .option('mode','FAILFAST') \\\n",
    "                                .load(\"/FileStore/tables/Flight_data.csv\")\n",
    "\n",
    "flight_df_new_schema.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eca93da-e904-4d01-af31-56084bd67ea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### READING CORRUPTED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8de1d21c-d96f-40de-89ff-a51258c0fc47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df = spark.read.format('csv') \\\n",
    "                  .option(\"header\", \"true\") \\\n",
    "                  .option(\"inferSchema\", \"true\") \\\n",
    "                  .option('mode','PERMISSIVE') \\\n",
    "                  .load(\"/FileStore/tables/Emp_pysp_details.csv\")\n",
    "\n",
    "\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b18ce9b0-802c-4232-a30c-fa46b85e46d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SHOW CORRUPTED RECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78727ce0-0be1-43d4-824e-4e0846e2e87d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_schema = StructType(\n",
    "    [\n",
    "        StructField('id', IntegerType(), True),\n",
    "        StructField('name ', StringType(), True),\n",
    "        StructField('age', DoubleType(), True),\n",
    "        StructField('salary', DoubleType(), True),\n",
    "        StructField('address', StringType(), True),\n",
    "        StructField('nominee', StringType(), True),\n",
    "        StructField('_corrupt_record', StringType(), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "emp_df_corrup = spark.read.format('csv') \\\n",
    "                  .option(\"header\", \"true\") \\\n",
    "                  .option(\"inferSchema\", \"false\") \\\n",
    "                  .schema(my_schema) \\\n",
    "                  .option('mode','PERMISSIVE') \\\n",
    "                  .load(\"/FileStore/tables/Emp_pysp_details.csv\")\n",
    "\n",
    "\n",
    "emp_df_corrup.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0172b8de-ab74-4caa-aa10-e5e490491922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### STORING BAD RECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe13ceed-61f4-46e5-9d73-862b0ffe6487",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_schema = StructType(\n",
    "    [\n",
    "        StructField('id', IntegerType(), True),\n",
    "        StructField('name ', StringType(), True),\n",
    "        StructField('age', DoubleType(), True),\n",
    "        StructField('salary', DoubleType(), True),\n",
    "        StructField('address', StringType(), True),\n",
    "        StructField('nominee', StringType(), True),\n",
    "        StructField('_corrupt_record', StringType(), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "emp_df_correct = spark.read.format('csv') \\\n",
    "                  .option(\"header\", \"true\") \\\n",
    "                  .option(\"inferSchema\", \"false\") \\\n",
    "                  .schema(my_schema) \\\n",
    "                  .option('badRecordsPath','/FileStore/tables/bad_records') \\\n",
    "                  .load(\"/FileStore/tables/Emp_pysp_details.csv\")\n",
    "\n",
    "\n",
    "emp_df_correct.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8081d3e-5dab-4afa-8e80-8bf8f5923b88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Read CSV",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}