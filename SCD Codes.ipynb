{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eb0b3e9-c435-4d45-97e5-c5468822af75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20b4bf21-4989-44aa-99f3-a87515a61fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **SCD - 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ab5d447-ae5b-4ad4-b985-d59c05536a52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "old_data = [(\"user1\",100),(\"user2\",200)]\n",
    "\n",
    "# create a dataframe\n",
    "old_df = spark.createDataFrame(old_data, [\"user\",\"sales\"])\n",
    "\n",
    "# create a delta table -- first Time\n",
    "old_df.write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/delta/Users\")\n",
    "\n",
    "# new data\n",
    "new_data = [(\"user3\",500),(\"user2\",400)]\n",
    "\n",
    "new_df = spark.createDataFrame(new_data, [\"user\",\"sales\"])\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# create a object of delta table to do advanced operation on the table\n",
    "delta_tbl = DeltaTable.forPath(spark, \"/FileStore/delta/Users\")\n",
    "\n",
    "# Perform upsert/merge on top of delta_table to avoid data corruption\n",
    "delta_tbl.alias(\"target\").merge(new_df.alias(\"source\"), \"target.user = source.user\") \\\n",
    "                        .whenMatchedUpdate(set = {\"sales\": \"source.sales\"}) \\\n",
    "                        .whenNotMatchedInsert(values = {\"user\":\"source.user\",\n",
    "                                                        \"sales\":\"source.sales\"}) \\\n",
    "                        .execute()\n",
    "\n",
    "# Read delta table data\n",
    "result_df = spark.read.format(\"delta\").load(\"/FileStore/delta/Users\")\n",
    "\n",
    "result_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d9ceafb-34dc-4aab-a49d-7039d2170dd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **SCD - 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d929fc21-7751-4ce1-8098-4805301757ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "SCD - 2 \n",
    "Steps: \n",
    "-         Step 1: Created a dataframe as source and loaded in delta table \n",
    "-         Step 2: In real time we'll have 2 more column start date and end date in table where start date will be filled by current date,\n",
    "-                 as we are doing on some dummy data, so instead of taking them in old data adding them during dataframe creation\n",
    "-         Step 3: We'll create another dataframe for new set data \n",
    "-         Step 4: Create a delta table object to perform any advance action on it:\n",
    ">                                If our delta table is saved on file based then use DeltaTable.forPath()\n",
    ">                                If our delta table is saved as sql managed table then DeltaTable.forName()\n",
    "-         Step 5: Now apply operations on it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "847d60bf-197f-4627-8dcb-a9f6a39c0a6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**DATA CHECK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fee0930-cf55-463c-acbc-1f7adb5b1b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from customer_table_scd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d97c191e-827d-4780-b62e-148db9d75608",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**CREATE DELTA TABLE FROM OLD DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "670392d1-38cc-4f1e-9846-4d5d08bacf2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "old_data = [(\"cust1\",\"Oin\",\"Kolkata\"),(\"cust2\",\"Kou\",\"Contai\")]\n",
    "\n",
    "# create a dataframe\n",
    "old_df = spark.createDataFrame(old_data, [\"cust_id\",\"name\",\"address\"]) \\\n",
    "              .withColumn(\"start_date\", current_date()) \\\n",
    "              .withColumn(\"end_date\", lit(None).cast(\"date\")) \\\n",
    "              .withColumn(\"flag\",lit(\"Y\"))\n",
    "\n",
    "# create a delta table -- first Time\n",
    "old_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"customer_table_scd2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0144479b-f938-4211-b660-19ca53b09b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**NEW INCOMING DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d178fdd-3601-4220-ae89-51b874ca64c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_data = [(\"cust1\",\"Oin\",\"Pune\",\"2025-05-25\"),(\"cust3\",\"Ree\",\"Bagnan\",\"2025-05-25\")]\n",
    "\n",
    "# create a dataframe\n",
    "new_df = spark.createDataFrame(new_data, [\"cust_id\",\"name\",\"address\",\"update_dt\"]) \\\n",
    "            .withColumn(\"update_dt\",col(\"update_dt\").cast(\"date\"))\n",
    "\n",
    "new_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43cb8d4a-c90b-4fa9-b34f-644ba9b853e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "FOR TESTING SCD FUNCTIONALITY CREATE DIFF DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f68c542-1326-491a-aacf-b8a78185f563",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_data = [(\"cust1\",\"Oin\",\"Kolkata\",\"2025-05-27\")]\n",
    "\n",
    "# create a dataframe\n",
    "new_df1 = spark.createDataFrame(new_data, [\"cust_id\",\"name\",\"address\",\"update_dt\"]) \\\n",
    "            .withColumn(\"update_dt\",col(\"update_dt\").cast(\"date\"))\n",
    "\n",
    "new_df1.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1233fb3-c56e-47a4-ad45-4075ed9c6e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**PERFORM SCD - 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09429b2e-3109-4258-ae9c-7574cc1254a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "USING PYSPARK CODE FOR UPDATE EXISTING RECORD AND BRAND NEW RECORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f7530b8-7b38-4289-ab77-716d63028ec7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table_scd2 = DeltaTable.forName(spark, \"customer_table_scd2\")\n",
    "\n",
    "delta_table_scd2.alias(\"Target\") \\\n",
    "        .merge(new_df.alias(\"Source\"), \"Source.cust_id == Target.cust_id and Target.flag = 'Y' and Target.end_date is null\") \\\n",
    "        .whenMatchedUpdate(\n",
    "            condition = \"Target.address != Source.address\",\n",
    "            set = {\n",
    "                \"end_date\" : col(\"Source.update_dt\").cast(\"date\"),\n",
    "                \"flag\" : lit(\"N\")\n",
    "            }\n",
    "        ) \\\n",
    "        .whenNotMatchedInsert(\n",
    "            values = {\n",
    "                \"cust_id\" : col(\"Source.cust_id\"),\n",
    "                \"name\" : col(\"Source.name\"),\n",
    "                \"address\" : col(\"Source.address\"),\n",
    "                \"start_date\" : col(\"Source.update_dt\"),\n",
    "                \"end_date\" : lit(None).cast(\"date\"),\n",
    "                \"flag\" : lit(\"Y\")\n",
    "            }\n",
    "        ) \\\n",
    "        .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a481f1d8-7019-4753-a652-cb498615fe30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "USING SQL CODE FOR UPDATE EXISTING RECORD AND BRAND NEW RECORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66756444-62fc-437d-998e-bed018ee0b3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df.createOrReplaceTempView(\"new_source\")\n",
    "new_df1.createOrReplaceTempView(\"new_source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15b8f9d3-46f3-4081-ac15-59ee097b2969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "merge into customer_table_scd2 as target\n",
    "using new_source as source\n",
    "on target.cust_id = source.cust_id and target.flag = 'Y' and isnull(target.end_date)\n",
    "\n",
    "when matched then\n",
    "update\n",
    "set \n",
    "target.end_date = source.update_dt,\n",
    "target.flag = 'N'\n",
    "\n",
    "when not matched then\n",
    "insert (cust_id,name,address,start_date,end_date,flag)\n",
    "values (source.cust_id,source.name,source.address,source.update_dt,null,'Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04fdfa4f-9f97-47f3-b139-8657d4cc3cc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "insert into customer_table_scd2(cust_id,name,address,start_date,end_date,flag) \n",
    "select\n",
    "  s.cust_id,\n",
    "  s.name,\n",
    "  s.address,\n",
    "  s.update_dt,\n",
    "  null as end_date,\n",
    "  'Y' as flag\n",
    "from new_source s\n",
    "join customer_table_scd2 t\n",
    "on s.cust_id = t.cust_id\n",
    "where  s.address <> t.address\n",
    "and t.flag = 'N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ced8469-133b-48fd-aafb-41a6e112e46c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY customer_table_scd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe580f7-a465-4dc9-b045-f7d14c41a0a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "RESTORE customer_table_scd2 TO VERSION AS OF 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bfa73bd-86d6-4839-a55a-0a9fa2dfa6ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3335234579731394,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD Codes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}